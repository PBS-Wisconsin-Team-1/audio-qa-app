{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"wav-example-file-download-1min.wav\"  \n",
    "\n",
    "# Load audio file\n",
    "# y = numpy array of audio samples\n",
    "# sr = sampling rate (samples per second)\n",
    "# sr=None keeps the original sampling rate\n",
    "y, sr = librosa.load(filename, sr=None)\n",
    "\n",
    "# Calculate duration of audio in seconds\n",
    "duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "print(f\"Audio duration: {duration:.2f} sec, Sampling rate: {sr} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot waveform\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title(\"Waveform\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spectrogram\n",
    "D = librosa.stft(y) # Compute Short-Time Fourier Transform (STFT)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max) # Convert amplitudes to decibels\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "# Display spectrogram with time on X-axis, frequency (log scale) on Y-axis\n",
    "librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB') # Show decibel scale\n",
    "plt.title('Spectrogram (dB)')\n",
    "plt.show()\n",
    "\n",
    "# A spectrogram lets us see distortions, hiss, pops, or unusual frequency spikes.\n",
    "# For example:\n",
    "# Sharp vertical lines = clicks/pops\n",
    "# High-frequency noise = hiss\n",
    "# Dropouts = silent horizontal bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMS /loudness over time\n",
    "rms = librosa.feature.rms(y=y)[0] # Compute Root Mean Square (RMS) amplitude\n",
    "frames = range(len(rms))  # Create indices for RMS frames\n",
    "t = librosa.frames_to_time(frames, sr=sr)  # Convert frame indices to time in seconds\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(t, rms) # Plot RMS over time\n",
    "plt.title(\"RMS Loudness over Time\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Low RMS = potential dropouts or silence\n",
    "# High RMS = spikes or clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distortion Detection\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the audio file\n",
    "y, sr = librosa.load(\"wav-example-file-download-1min.wav\", sr=None)\n",
    "\n",
    "# Normalize audio (so amplitude is between -1 and 1)\n",
    "y = y / np.max(np.abs(y))\n",
    "\n",
    "# Detect clipping\n",
    "clip_threshold = 0.98  # near max amplitude\n",
    "clipped = np.abs(y) > clip_threshold\n",
    "clip_ratio = np.sum(clipped) / len(y)\n",
    "\n",
    "print(f\"Clipping detected in {clip_ratio * 100:.2f}% of samples\")\n",
    "\n",
    "\n",
    "# Identify distorted segments\n",
    "# Convert boolean array to time points\n",
    "clipped_frames = np.where(clipped)[0]\n",
    "clipped_times = clipped_frames / sr\n",
    "\n",
    "# Spectral analysis for extra confirmation\n",
    "spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "frames = range(len(spectral_centroid))\n",
    "t = librosa.frames_to_time(frames, sr=sr)\n",
    "\n",
    "# Mark areas of potential distortion where centroid is very high\n",
    "threshold = np.mean(spectral_centroid) + 2 * np.std(spectral_centroid)\n",
    "distorted_regions = t[spectral_centroid > threshold]\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y, sr=sr, alpha=0.6)\n",
    "plt.scatter(clipped_times, np.ones_like(clipped_times)*0.9, color='r', s=5, label='Clipping')\n",
    "plt.title(\"Distortion Detection: Clipped Samples Highlighted\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"=== Distortion Summary ===\")\n",
    "print(f\"Sampling Rate: {sr} Hz\")\n",
    "print(f\"Duration: {librosa.get_duration(y=y, sr=sr):.2f} seconds\")\n",
    "print(f\"Total Clipped Samples: {np.sum(clipped)}\")\n",
    "print(f\"Distortion Likely Around: {distorted_regions[:10]} seconds (first 10 shown)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distortion Detection\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the audio file\n",
    "y, sr = librosa.load(\"Audio Recording on Wednesday afternoon.wav\", sr=None)\n",
    "\n",
    "# Normalize audio (so amplitude is between -1 and 1)\n",
    "y = y / np.max(np.abs(y))\n",
    "\n",
    "# Detect clipping\n",
    "clip_threshold = 0.98  # near max amplitude\n",
    "clipped = np.abs(y) > clip_threshold\n",
    "clip_ratio = np.sum(clipped) / len(y)\n",
    "\n",
    "print(f\"Clipping detected in {clip_ratio * 100:.2f}% of samples\")\n",
    "\n",
    "\n",
    "# Identify distorted segments\n",
    "# Convert boolean array to time points\n",
    "clipped_frames = np.where(clipped)[0]\n",
    "clipped_times = clipped_frames / sr\n",
    "\n",
    "# Spectral analysis for extra confirmation\n",
    "spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "frames = range(len(spectral_centroid))\n",
    "t = librosa.frames_to_time(frames, sr=sr)\n",
    "\n",
    "# Mark areas of potential distortion where centroid is very high\n",
    "threshold = np.mean(spectral_centroid) + 2 * np.std(spectral_centroid)\n",
    "distorted_regions = t[spectral_centroid > threshold]\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y, sr=sr, alpha=0.6)\n",
    "plt.scatter(clipped_times, np.ones_like(clipped_times)*0.9, color='r', s=5, label='Clipping')\n",
    "plt.title(\"Distortion Detection: Clipped Samples Highlighted\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"=== Distortion Summary ===\")\n",
    "print(f\"Sampling Rate: {sr} Hz\")\n",
    "print(f\"Duration: {librosa.get_duration(y=y, sr=sr):.2f} seconds\")\n",
    "print(f\"Total Clipped Samples: {np.sum(clipped)}\")\n",
    "print(f\"Distortion Likely Around: {distorted_regions[:10]} seconds (first 10 shown)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Audio)",
   "language": "python",
   "name": "audioenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
